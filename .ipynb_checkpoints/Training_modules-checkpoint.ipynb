{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Numpy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# display Python object in all frontends\n",
    "from IPython.display import display\n",
    "\n",
    "# store elements as dictionary keys and their counts as dictionary values\n",
    "from collections import Counter\n",
    "\n",
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import SVM classifier \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Function for creating model pipelines - sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Function for creating model pipelines - imblearn\n",
    "from imblearn.pipeline import make_pipeline as imbl_pipe\n",
    "\n",
    "# Over-sampling using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# set class weights for imbalaced datasets\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Ignore some warning messages\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning, ConvergenceWarning, UndefinedMetricWarning\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.simplefilter(action='ignore', category=DataConversionWarning)\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tune_CV(pipe, scorer):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    print('skf calculated')\n",
    "    # Create empty dictionary called fitted_models\n",
    "    fit_models = {}\n",
    "\n",
    "    # Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "    for name, pipeline in pipe.items():\n",
    "        print(name, 'Searching best Hyperparametres')\n",
    "        # Create cross-validation object from pipeline and hyperparameters\n",
    "        model = GridSearchCV(pipeline, hyperparameters[name], cv=skf,\n",
    "                             scoring=scorer, iid=True, n_jobs=-1)\n",
    "        print(name, 'fitting')\n",
    "        # Fit model on X_train, y_train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store model in fitted_models[name]\n",
    "        fit_models[name] = model\n",
    "\n",
    "        # Print '{name} has been fitted'\n",
    "        print(name, 'has been fitted')\n",
    "    return fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(fit_models):\n",
    "    lst = []\n",
    "    for name, model in fit_models.items():\n",
    "        pred = model.predict(X_test)\n",
    "        lst.append([name, model.best_score_, f1_score(y_test, pred, average='macro'),\n",
    "                    accuracy_score(y_test, pred)])\n",
    "\n",
    "    eval_df = pd.DataFrame(lst, columns=['model', 'CV_score', 'f1_macro', 'accuracy'])\n",
    "    eval_df.set_index('model', inplace = True)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_plot(eval_df):\n",
    "    #eval_df = evaluation(fit_models)\n",
    "    eval_dfp = eval_df.reset_index()\n",
    "    eval_dfp = pd.melt(eval_dfp,id_vars='model',var_name='metrics', value_name='score')\n",
    "\n",
    "    sns.catplot(x='model', y='score', hue='metrics',data=eval_dfp, kind='bar',\n",
    "                palette={'CV_score' : 'red', 'f1_macro' : 'orange',\n",
    "                          'accuracy' : 'royalblue'})\n",
    "    plt.title('Evaluation Metrics', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel('Model', size=12)\n",
    "    plt.ylabel('Score', size=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat_w_and_wo_norm(fit_models, model_id, color):\n",
    "    # Plot confusion matrix heatmaps\n",
    "    pred = fit_models[model_id].predict(X_test)\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    f.suptitle(models[model_id], fontsize=14)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    # confusion matrix without normalization\n",
    "    mat = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(mat,\n",
    "                annot=True,\n",
    "                annot_kws=dict(fontsize=14),\n",
    "                fmt='d',\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "                cmap=color,\n",
    "                ax=ax1)\n",
    "\n",
    "    ax1.set_xticklabels(labels=target_names)\n",
    "    ax1.set_yticklabels(labels=target_names, va='center')\n",
    "    ax1.set_title('Confusion Matrix w/o Normalization')\n",
    "    ax1.set_xlabel('Predicted Labels', size=12)\n",
    "    ax1.set_ylabel('True Labels', size=12)\n",
    "\n",
    "    # normalized confusion matrix\n",
    "    matn = mat / mat.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(matn,\n",
    "                annot=True,\n",
    "                annot_kws=dict(fontsize=14),\n",
    "                fmt='.2f',\n",
    "                cbar=True,\n",
    "                square=True,\n",
    "                cmap=color,\n",
    "                vmin = 0,\n",
    "                vmax = 1,\n",
    "                ax=ax2)\n",
    "\n",
    "    ax2.set_xticklabels(labels=target_names)\n",
    "    ax2.set_yticklabels(labels=target_names, va='center')\n",
    "    ax2.set_title('Normalized Confusion Matrix')\n",
    "    ax2.set_xlabel('Predicted Label', size=12)\n",
    "    ax2.set_ylabel('True Label', size=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norm_conf_matrices(fit_models, color):\n",
    "    # Prepare list of coordintaes for axes\n",
    "    lt = []\n",
    "    col = 2\n",
    "    row = int(len(fit_models) / col)\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            lt.append([r, c])\n",
    "\n",
    "    # Create figure and subplots\n",
    "    figs_y = row * 4\n",
    "    f, axs = plt.subplots(row, col, figsize=(10, figs_y))\n",
    "    f.suptitle('Normalized Confusion Matrices', fontsize=14)\n",
    "    f.subplots_adjust(top=0.94, wspace=0.90, hspace=0.2)\n",
    "\n",
    "    i = 0\n",
    "    # Loop for each fitted model        \n",
    "    for id, model in fit_models.items():\n",
    "        pred = model.predict(X_test)\n",
    "        name = models[id]\n",
    "        r = lt[i][0]\n",
    "        c = lt[i][1]\n",
    "        i += 1\n",
    "\n",
    "        mat = confusion_matrix(y_test, pred)    \n",
    "        # normalized confusion matrix\n",
    "        matn = mat / mat.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(matn,\n",
    "                    annot=True,\n",
    "                    annot_kws=dict(fontsize=14),\n",
    "                    fmt='.2f',\n",
    "                    cbar=False,\n",
    "                    square=True,\n",
    "                    cmap=color,\n",
    "                    vmin = 0,\n",
    "                    vmax = 1,\n",
    "                    #cbar_kws = {'shrink' : 0.85},\n",
    "                    ax=axs[r, c])\n",
    "\n",
    "        axs[r, c].set_xticklabels(labels=target_names)\n",
    "        axs[r, c].set_yticklabels(labels=target_names, va='center')\n",
    "        axs[r, c].set_title(name)\n",
    "        axs[r, c].set_xlabel('Predicted Label', size=12)\n",
    "        axs[r, c].set_ylabel('True Label', size=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_rep_cm(fit_models, model_id):\n",
    "    # Predict classes using model_id\n",
    "    pred = fit_models[model_id].predict(X_test)\n",
    "    print()\n",
    "    print('\\t', models[model_id])\n",
    "    print('\\t', '='*len(models[model_id]))\n",
    "\n",
    "    # Display confusion matrix for y_test and pred\n",
    "    conf_df = pd.DataFrame(confusion_matrix(y_test, pred), columns=target_names, index=target_names)\n",
    "    conf_df.index.name = 'True Labels'\n",
    "    conf_df = conf_df.rename_axis('Predicted Labels', axis='columns')\n",
    "    display(conf_df)\n",
    "\n",
    "    # Display classification report\n",
    "    print()\n",
    "    print(classification_report(y_test, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_hyp_param(fit_models):\n",
    "    # Display best_params_ for each fitted model\n",
    "\n",
    "    # Initialize empty dataframe\n",
    "    bp_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through all fitted models\n",
    "    for name, model in fit_models.items():\n",
    "        # Dictionary of best_params\n",
    "        d = model.best_params_\n",
    "        # Model name from model dictionary\n",
    "        model_name = models[name]\n",
    "\n",
    "        # Create dataframe for best_params_dictionary\n",
    "        bp_dft = pd.DataFrame.from_dict(d, orient='index', columns=['Value'])\n",
    "        # Insert the column 'Model'\n",
    "        bp_dft.insert(0, 'Model', model_name)\n",
    "        # Concatenate previous dataframe with new one from this run\n",
    "        bp_df = pd.concat([bp_df, bp_dft])\n",
    "\n",
    "    # Finalize the output of the dataframe\n",
    "    bp_df.reset_index(inplace=True)\n",
    "    bp_df.set_index('Model', inplace = True)\n",
    "    bp_df.rename(columns={'index' : 'Hyperparameter'}, inplace=True)\n",
    "    return bp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
